{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datasets\n",
    "import csv\n",
    "import models\n",
    "import utils\n",
    "from utils import *\n",
    "from torchvision import transforms\n",
    "from mmcv.runner import load_checkpoint\n",
    "import csv\n",
    "import statistics as st\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.segmentation import watershed\n",
    "# from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "import random\n",
    "from models.sam2.build_sam import build_sam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf87f33726901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185b1ce94fa4b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def calculate_metrics(pred, gt):\n",
    "    pred = pred.flatten()\n",
    "    gt = gt.flatten()\n",
    "\n",
    "    tp = np.sum((pred == 1) & (gt == 1))\n",
    "    fp = np.sum((pred == 1) & (gt == 0))\n",
    "    fn = np.sum((pred == 0) & (gt == 1))\n",
    "    tn = np.sum((pred == 0) & (gt == 0))\n",
    "\n",
    "    epsilon = 1e-7  # To avoid division by zero\n",
    "\n",
    "    dice = (2 * tp + epsilon) / (2 * tp + fp + fn + epsilon)\n",
    "    iou = (tp + epsilon) / (tp + fp + fn + epsilon)\n",
    "    precision = (tp + epsilon) / (tp + fp + epsilon)\n",
    "    recall = (tp + epsilon) / (tp + fn + epsilon)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "\n",
    "    return dice, iou, precision, recall, f1\n",
    "\n",
    "\n",
    "def save_metrics_to_file(all_metrics, average_metrics, iou_list, dice_list, precision_list, recall_list, f1_list,\n",
    "                         filename='all_metrics.txt'):\n",
    "    \"\"\"\n",
    "    Save individual image metrics and also lists of metrics at the end.\n",
    "\n",
    "    all_metrics: List of dictionaries containing metrics per image\n",
    "    average_metrics: Dictionary containing average metrics\n",
    "    iou_list, dice_list, precision_list, recall_list, f1_list: Lists to store metrics for all images\n",
    "    filename: Path of the file to save the metrics\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['filename', 'dice', 'iou', 'precision', 'recall', 'f1'])\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Save individual image metrics\n",
    "        for metric in all_metrics:\n",
    "            writer.writerow(metric)\n",
    "\n",
    "        # Add a separator\n",
    "        writer.writerow({k: '-' * 10 for k in writer.fieldnames})\n",
    "\n",
    "        # Add average metrics\n",
    "        writer.writerow({\n",
    "            'filename': 'AVERAGE',\n",
    "            'dice': f\"{average_metrics['dice']:.4f}\",\n",
    "            'iou': f\"{average_metrics['iou']:.4f}\",\n",
    "            'precision': f\"{average_metrics['precision']:.4f}\",\n",
    "            'recall': f\"{average_metrics['recall']:.4f}\",\n",
    "            'f1': f\"{average_metrics['f1']:.4f}\"\n",
    "        })\n",
    "\n",
    "        # Add lists of all metrics at the end\n",
    "        f.write(\"\\n\\nIoU List:\\n\")\n",
    "        f.write(\", \".join([f\"{iou:.4f}\" for iou in iou_list]) + \"\\n\")\n",
    "\n",
    "        f.write(\"Dice List:\\n\")\n",
    "        f.write(\", \".join([f\"{dice:.4f}\" for dice in dice_list]) + \"\\n\")\n",
    "\n",
    "        f.write(\"Precision List:\\n\")\n",
    "        f.write(\", \".join([f\"{precision:.4f}\" for precision in precision_list]) + \"\\n\")\n",
    "\n",
    "        f.write(\"Recall List:\\n\")\n",
    "        f.write(\", \".join([f\"{recall:.4f}\" for recall in recall_list]) + \"\\n\")\n",
    "\n",
    "        f.write(\"F1 List:\\n\")\n",
    "        f.write(\", \".join([f\"{f1:.4f}\" for f1 in f1_list]) + \"\\n\")\n",
    "\n",
    "\n",
    "def eval_psnr(args, loader, model, star_writer, save_dir, verbose=False, org_image_size=(4096, 4096)):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    metric_averagers = {\n",
    "        'dice': utils.Averager(),\n",
    "        'iou': utils.Averager(),\n",
    "        'precision': utils.Averager(),\n",
    "        'recall': utils.Averager(),\n",
    "        'f1': utils.Averager()\n",
    "    }\n",
    "\n",
    "    pbar = tqdm(loader, leave=False, desc='val')\n",
    "\n",
    "    all_metrics = []\n",
    "\n",
    "    for idx, batch in enumerate(pbar):\n",
    "        for k, v in batch.items():\n",
    "            if k != 'filename':\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "        inp = batch['inp']\n",
    "        gt = batch['gt']\n",
    "        filenames = batch['filename']  # Get filenames from the batch\n",
    "\n",
    "        pred = torch.sigmoid(model.infer(inp))\n",
    "        pred = (pred > 0.5).float()\n",
    "\n",
    "        for i in range(inp.shape[0]):\n",
    "            # Convert to numpy arrays\n",
    "            pred_np = pred[i].cpu().numpy()\n",
    "            gt_np = gt[i].cpu().numpy()\n",
    "\n",
    "            dice, iou, precision, recall, f1 = calculate_metrics(pred_np, gt_np)\n",
    "            metrics = {\n",
    "                'filename': filenames[i],\n",
    "                'dice': dice,\n",
    "                'iou': iou,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            }\n",
    "            all_metrics.append(metrics)\n",
    "\n",
    "            for key, value in metrics.items():\n",
    "                if key != 'filename':\n",
    "                    metric_averagers[key].add(value, 1)\n",
    "\n",
    "            generate_output(args, inp[i], gt[i], pred[i], filenames[i], star_writer, output_dir=save_dir,\n",
    "                            org_image_size=org_image_size)\n",
    "\n",
    "        if verbose:\n",
    "            pbar.set_description('val Dice: {:.4f}, IoU: {:.4f}, F1: {:.4f}'.format(\n",
    "                metric_averagers['dice'].item(),\n",
    "                metric_averagers['iou'].item(),\n",
    "                metric_averagers['f1'].item()\n",
    "            ))\n",
    "\n",
    "    average_metrics = {key: averager.item() for key, averager in metric_averagers.items()}\n",
    "    return all_metrics, average_metrics\n",
    "\n",
    "\n",
    "def count_circles_hough(mask, dp=1.2, min_dist=20, param1=50, param2=30, min_radius=5, max_radius=50):\n",
    "    \"\"\"\n",
    "    Count circular particles in a mask using the Hough Circle Transform.\n",
    "\n",
    "    Parameters:\n",
    "    - mask: The mask image where circles are to be counted.\n",
    "    - dp: Inverse ratio of the accumulator resolution to the image resolution (default: 1.2).\n",
    "    - min_dist: Minimum distance between detected centers (default: 20).\n",
    "    - param1: Higher threshold for the Canny edge detector (default: 50).\n",
    "    - param2: Accumulator threshold for the circle centers at the detection stage (default: 30).\n",
    "    - min_radius: Minimum circle radius (default: 5).\n",
    "    - max_radius: Maximum circle radius (default: 50).\n",
    "\n",
    "    Returns:\n",
    "    - The number of detected circles.\n",
    "    \"\"\"\n",
    "    circles = cv2.HoughCircles(mask, cv2.HOUGH_GRADIENT, dp=dp, minDist=min_dist,\n",
    "                               param1=param1, param2=param2, minRadius=min_radius, maxRadius=max_radius)\n",
    "    if circles is not None:\n",
    "        circles = np.round(circles[0, :]).astype(\"int\")\n",
    "        return len(circles)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def detect_circular_particles(mask_path, particle_radius=28):\n",
    "    \"\"\"\n",
    "    Enhanced particle detection optimized for dense, connected regions.\n",
    "    \"\"\"\n",
    "    # Read and preprocess mask\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.resize(mask, (1024, 1024))\n",
    "    if mask is None:\n",
    "        raise ValueError(f\"Could not read mask from {mask_path}\")\n",
    "\n",
    "    binary_mask = (mask > 127).astype(np.uint8)\n",
    "\n",
    "    # Step 1: Enhanced Distance Transform\n",
    "    distance = ndi.distance_transform_edt(binary_mask)\n",
    "\n",
    "    # Step 2: Multi-scale Detection with Different Approaches\n",
    "    all_coordinates = []\n",
    "\n",
    "    # A) Multiple threshold levels with different sensitivities\n",
    "    threshold_levels = [\n",
    "        particle_radius * 0.05,  # Very sensitive\n",
    "        particle_radius * 0.10,\n",
    "        particle_radius * 0.15,\n",
    "        particle_radius * 0.20,\n",
    "        particle_radius * 0.25\n",
    "    ]\n",
    "\n",
    "    min_distances = [\n",
    "        int(particle_radius * 0.3),  # More aggressive\n",
    "        int(particle_radius * 0.4),\n",
    "        int(particle_radius * 0.5)\n",
    "    ]\n",
    "\n",
    "    # Combine different thresholds and distances\n",
    "    for threshold in threshold_levels:\n",
    "        for min_dist in min_distances:\n",
    "            coordinates = peak_local_max(\n",
    "                distance,\n",
    "                min_distance=min_dist,\n",
    "                threshold_abs=threshold,\n",
    "                exclude_border=False,\n",
    "            )\n",
    "            all_coordinates.extend(coordinates)\n",
    "\n",
    "    # B) Use morphological operations to separate connected components\n",
    "    #     kernel_size = int(particle_radius * 0.3)\n",
    "    kernel_size = int(particle_radius * 0.8)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    eroded = cv2.erode(binary_mask, kernel, iterations=1)\n",
    "\n",
    "    # Find additional centers from eroded image\n",
    "    dist_eroded = ndi.distance_transform_edt(eroded)\n",
    "    #     coordinates_eroded = peak_local_max(\n",
    "    #         dist_eroded,\n",
    "    #         min_distance=int(particle_radius * 0.4),\n",
    "    #         threshold_abs=particle_radius * 0.1,\n",
    "    #         exclude_border=False,\n",
    "    #     )\n",
    "    #     coordinates_eroded = peak_local_max(\n",
    "    #         dist_eroded,\n",
    "    #         min_distance=int(particle_radius * 0.05),\n",
    "    #         threshold_abs=particle_radius * 0.05,\n",
    "    #         exclude_border=False,\n",
    "    #     )\n",
    "\n",
    "    coordinates_eroded = peak_local_max(\n",
    "        dist_eroded,\n",
    "        min_distance=int(particle_radius * 0.1),\n",
    "        threshold_abs=particle_radius * 0.1,\n",
    "        exclude_border=False,\n",
    "    )\n",
    "    all_coordinates.extend(coordinates_eroded)\n",
    "\n",
    "    # C) Use circular Hough transform for additional detection\n",
    "    circles = cv2.HoughCircles(\n",
    "        binary_mask,\n",
    "        cv2.HOUGH_GRADIENT,\n",
    "        dp=1,\n",
    "        minDist=int(particle_radius * 0.8),\n",
    "        param1=50,\n",
    "        param2=15,\n",
    "        minRadius=int(particle_radius * 0.6),\n",
    "        maxRadius=int(particle_radius * 1.4)\n",
    "    )\n",
    "\n",
    "    if circles is not None:\n",
    "        circles = np.round(circles[0, :]).astype(int)\n",
    "        for circle in circles:\n",
    "            all_coordinates.append([circle[1], circle[0]])  # y, x coordinates\n",
    "\n",
    "    # Remove duplicates with a distance threshold\n",
    "    filtered_coordinates = []\n",
    "    distance_threshold = particle_radius * 0.5\n",
    "\n",
    "    all_coordinates = np.unique(all_coordinates, axis=0)\n",
    "    for coord in all_coordinates:\n",
    "        if not any(np.hypot(coord[0] - existing[0], coord[1] - existing[1]) < distance_threshold\n",
    "                   for existing in filtered_coordinates):\n",
    "            filtered_coordinates.append(coord)\n",
    "\n",
    "    # Create markers for watershed\n",
    "    markers = np.zeros_like(binary_mask, dtype=np.int32)\n",
    "    for i, coord in enumerate(filtered_coordinates, start=1):\n",
    "        markers[coord[0], coord[1]] = i\n",
    "\n",
    "    # Apply watershed\n",
    "    labels = watershed(-distance, markers, mask=binary_mask)\n",
    "\n",
    "    # Process regions and validate particles\n",
    "    valid_centers = []\n",
    "    processed_regions = np.zeros_like(binary_mask)\n",
    "\n",
    "    for label_idx in range(1, labels.max() + 1):\n",
    "        particle_mask = (labels == label_idx).astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(\n",
    "            particle_mask,\n",
    "            cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        if not contours:\n",
    "            continue\n",
    "\n",
    "        contour = contours[0]\n",
    "        area = cv2.contourArea(contour)\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0\n",
    "\n",
    "        expected_area = np.pi * particle_radius * particle_radius\n",
    "        #         min_area = expected_area * 0.2  # More permissive\n",
    "        min_area = expected_area * 0.05  # More permissive\n",
    "        max_area = expected_area * 2.5  # More permissive\n",
    "\n",
    "        is_circular = circularity > 0.4  # More permissive\n",
    "        is_valid_size = min_area <= area <= max_area\n",
    "\n",
    "        if is_valid_size and (is_circular or area < expected_area * 1.3):\n",
    "            M = cv2.moments(particle_mask)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cy = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cx = int(M[\"m01\"] / M[\"m00\"])\n",
    "                valid_centers.append((cx, cy))\n",
    "                processed_regions = cv2.bitwise_or(processed_regions, particle_mask)\n",
    "\n",
    "    # Final pass for missed regions\n",
    "    remaining_mask = cv2.bitwise_and(binary_mask, cv2.bitwise_not(processed_regions))\n",
    "    if np.any(remaining_mask):\n",
    "        contours, _ = cv2.findContours(\n",
    "            remaining_mask,\n",
    "            cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area >= min_area:\n",
    "                M = cv2.moments(contour)\n",
    "                if M[\"m00\"] != 0:\n",
    "                    cy = int(M[\"m10\"] / M[\"m00\"])\n",
    "                    cx = int(M[\"m01\"] / M[\"m00\"])\n",
    "                    is_new_center = all(\n",
    "                        np.hypot(cx - c[0], cy - c[1]) > particle_radius * 0.8\n",
    "                        for c in valid_centers\n",
    "                    )\n",
    "                    if is_new_center:\n",
    "                        valid_centers.append((cx, cy))\n",
    "\n",
    "    return binary_mask, distance, labels, valid_centers\n",
    "\n",
    "\n",
    "def generate_output(args, org_image, gt, pred_mask, filename, star_writer, output_dir, org_image_size=(4096, 4096)):\n",
    "    height, width = org_image.shape[1], org_image.shape[2]\n",
    "\n",
    "    original_image = org_image.detach().cpu().permute(1, 2, 0).numpy()\n",
    "    original_image = cv2.resize(original_image,\n",
    "                                (config['model']['args']['inp_size'], config['model']['args']['inp_size']))\n",
    "    original_image = (original_image - original_image.min()) / (original_image.max() - original_image.min())\n",
    "\n",
    "    original_mask = gt.detach().cpu().squeeze().numpy()\n",
    "    original_mask_8_bit = (original_mask * 255).astype(np.uint8)\n",
    "    # Count circles in the ground truth mask using Hough Circle Transform\n",
    "    ground_truth_circular_count = count_circles_hough(original_mask_8_bit)\n",
    "\n",
    "    predicted_mask = pred_mask.detach().cpu().numpy().reshape(config['model']['args']['inp_size'],\n",
    "                                                              config['model']['args']['inp_size'])\n",
    "\n",
    "    predicted_mask = np.clip(predicted_mask, 0, 1)\n",
    "\n",
    "    # add this lines of code to get the coordinates for CryoSparc\n",
    "    predicted_mask = np.rot90(predicted_mask, k=3)\n",
    "    predicted_mask = predicted_mask.T\n",
    "    predicted_mask_8bit = (predicted_mask * 255).astype(np.uint8)\n",
    "\n",
    "    # Save temporary mask image\n",
    "    temp_mask_path = os.path.join(output_dir, \"temp_mask.png\")\n",
    "    cv2.imwrite(temp_mask_path, predicted_mask_8bit)\n",
    "\n",
    "    binary_mask, distance, labels, valid_centers = detect_circular_particles(\n",
    "        temp_mask_path,\n",
    "        particle_radius=args.particle_radius\n",
    "    )\n",
    "\n",
    "    # Remove temporary file\n",
    "    os.remove(temp_mask_path)\n",
    "\n",
    "    filename_base = filename.rsplit('_', 1)[0]\n",
    "    filename_mrc = filename_base + '.mrc'\n",
    "    #    filename_mrc = filename + '.mrc'\n",
    "\n",
    "    #    original_height, original_width = org_image_size[0], org_image_size[1]\n",
    "    original_width, original_height = org_image_size[0], org_image_size[1]\n",
    "\n",
    "    # Compute scaling factors for x and y axes\n",
    "    scale_x = original_width / width\n",
    "    scale_y = original_height / height\n",
    "\n",
    "    original_image = np.rot90(original_image, k=3)\n",
    "    original_image = original_image.T\n",
    "    original_image = np.transpose(original_image, (1, 2, 0))\n",
    "\n",
    "    original_mask = np.rot90(original_mask, k=3)\n",
    "    original_mask = original_mask.T\n",
    "\n",
    "    # Create visualization image\n",
    "\n",
    "    mask_bgr = cv2.cvtColor(predicted_mask_8bit, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Draw circles and write to star file\n",
    "    circle_count = 0\n",
    "    for center in valid_centers:\n",
    "        cx, cy = center\n",
    "\n",
    "        # Draw circle on visualization\n",
    "        cv2.circle(mask_bgr, (cy, cx), args.particle_radius, (0, 0, 255), 3)\n",
    "\n",
    "        # Scale coordinates to original image size\n",
    "        scaled_x = int(round(cy * scale_x))\n",
    "        scaled_y = int(round(cx * scale_y))\n",
    "        scaled_diameter = int(round(2 * args.particle_radius * (scale_x + scale_y) / 2))\n",
    "\n",
    "        # Write to star file\n",
    "        star_writer.writerow([\n",
    "            filename_mrc,\n",
    "            scaled_x,\n",
    "            scaled_y,\n",
    "            scaled_diameter\n",
    "        ])\n",
    "        circle_count += 1\n",
    "\n",
    "    mask_rgb = cv2.cvtColor(mask_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(original_mask, cmap='gray')\n",
    "    axes[1].set_title(f\"Original Mask:{ground_truth_circular_count}\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(predicted_mask, cmap='gray')\n",
    "    axes[2].set_title(\"Predicted Mask\")\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    axes[3].imshow(mask_rgb)\n",
    "    axes[3].axis('off')\n",
    "    axes[3].set_title(f\"Predicted Mask with circles: {circle_count}\")\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.savefig(f\"{save_dir}/{filename}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def override_config(config, args):\n",
    "    \"\"\"Override parts of config with user args (if provided).\"\"\"\n",
    "    # Training dataset overrides\n",
    "    if args.test_images is not None:\n",
    "        config[\"test_dataset\"][\"dataset\"][\"args\"][\"root_path_1\"] = args.test_images\n",
    "    if args.test_labels is not None:\n",
    "        config[\"test_dataset\"][\"dataset\"][\"args\"][\"root_path_2\"] = args.test_labels\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf177c2d2e3996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Metrics:\n",
      "Dice: 0.7758\n",
      "Iou: 0.6359\n",
      "Precision: 0.6787\n",
      "Recall: 0.9119\n",
      "F1: 0.7758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--config', default=\"configs/cod-sam-vit-l.yaml\")\n",
    "    # parser.add_argument('--model',\n",
    "    #                     default=\"./checkpoints/5_shot/10028_few_shot/model_epoch_best.pth\", help='path to  the trained model')\n",
    "    parser.add_argument('--model',\n",
    "                        default=\"/cluster/pixstor/xudong-lab/biplab/CryoPPP_SAM2/checkpoints/5_shot/10028_5000Epochs/model_epoch_best.pth\", help='path to  the trained model')\n",
    "    parser.add_argument('--shot', type=int, default=5, choices=[1, 5, 10], required=False,\n",
    "                        help='Number of shots for few-shot training (1, 5, or 10)')\n",
    "    parser.add_argument('--protein_name', default='10028', help='EMPAIR ID of the protein')\n",
    "    parser.add_argument('--particle_radius', type=int, default=28, help='radius of the particle for given protein having images of size (1024, 1024)')\n",
    "    parser.add_argument('--org_image_size', type=int, nargs=2, default=[4096, 4096], help='size of original images')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='./data_outputs',\n",
    "                        help='save the output images and information')\n",
    "\n",
    "    # Arguments for training and validation images and labels\n",
    "    parser.add_argument('--test_images', type=str,\n",
    "                        default=\"./data/test/images/\",\n",
    "                        help=\"Path to test/images\")\n",
    "    parser.add_argument('--test_labels', type=str,\n",
    "                        default=\"./data/test/labels/\",\n",
    "                        help=\"Path to test/labels\")\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    with open(args.config, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    config = override_config(config, args)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    args.device = device\n",
    "\n",
    "    # Access the image size as a tuple\n",
    "    image_size = tuple(args.org_image_size)\n",
    "\n",
    "    spec = config['test_dataset']\n",
    "    dataset = datasets.make(spec['dataset'])\n",
    "    dataset = datasets.make(spec['wrapper'], args={'dataset': dataset})\n",
    "    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n",
    "                        num_workers=1, shuffle=False)\n",
    "\n",
    "    model = models.make(config['model'])\n",
    "    sam_checkpoint = torch.load(args.model, map_location='cpu')\n",
    "\n",
    "    # Extract model state dict from checkpoint\n",
    "    if isinstance(sam_checkpoint, dict) and 'model' in sam_checkpoint:\n",
    "        # If checkpoint is a dictionary containing 'model' key\n",
    "        model_state_dict = sam_checkpoint['model']\n",
    "    else:\n",
    "        # If checkpoint is directly the state dict\n",
    "        model_state_dict = sam_checkpoint\n",
    "\n",
    "    # Load the model state dict\n",
    "    model.load_state_dict(model_state_dict, strict=True)\n",
    "\n",
    "    shot_dir = f'{args.shot}_shot'\n",
    "    protein_path = os.path.join(args.output_dir, shot_dir, args.protein_name)\n",
    "    os.makedirs(protein_path, exist_ok=True)\n",
    "\n",
    "    save_dir = os.path.join(protein_path, 'micrographs_outputs')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    star_file = os.path.join(protein_path, f'{args.protein_name}_star_file.star')\n",
    "\n",
    "    with open(star_file, 'w', newline='') as star_file:\n",
    "        star_writer = csv.writer(star_file, delimiter=' ')\n",
    "        star_writer.writerow([])\n",
    "        star_writer.writerow([\"data_\"])\n",
    "        star_writer.writerow([])\n",
    "        star_writer.writerow([\"loop_\"])\n",
    "        star_writer.writerow([\"_rlnMicrographName\", \"#1\"])\n",
    "        star_writer.writerow([\"_rlnCoordinateX\", \"#2\"])\n",
    "        star_writer.writerow([\"_rlnCoordinateY\", \"#3\"])\n",
    "        star_writer.writerow([\"_rlnDiameter\", \"#4\"])\n",
    "\n",
    "        all_metrics, average_metrics = eval_psnr(args, loader=loader, model=model, star_writer=star_writer,\n",
    "                                                 verbose=True, save_dir=save_dir, org_image_size=image_size)\n",
    "\n",
    "        # Lists to hold the metrics for all images\n",
    "        iou_list = []\n",
    "        dice_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "\n",
    "        # Populate the metric lists from the per-image metrics\n",
    "        for metric in all_metrics:\n",
    "            iou_list.append(metric['iou'])\n",
    "            dice_list.append(metric['dice'])\n",
    "            precision_list.append(metric['precision'])\n",
    "            recall_list.append(metric['recall'])\n",
    "            f1_list.append(metric['f1'])\n",
    "\n",
    "        # Save individual image metrics and lists of metrics to the same file\n",
    "        save_metrics_to_file(all_metrics, average_metrics, iou_list, dice_list, precision_list, recall_list,\n",
    "                             f1_list, os.path.join(protein_path, 'all_metrics.txt'))\n",
    "\n",
    "        # Print average metrics\n",
    "        print('Average Test Metrics:')\n",
    "        for key, value in average_metrics.items():\n",
    "            print(f'{key.capitalize()}: {value:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CryoPPP (conda)",
   "language": "python",
   "name": "cryoppp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
